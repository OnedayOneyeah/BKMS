{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Lyrics Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloe/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "data = pd.read_csv('./spotify_data.csv', index_col=[0])\n",
    "lyrics_data = pd.DataFrame(data['lyrics'])\n",
    "lyrics_data['embedding'] = ''\n",
    "lyrics_data['lyrics'] = lyrics_data['lyrics'].apply(lambda e: e.replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! hehehe\n"
     ]
    }
   ],
   "source": [
    "a = 'Hi there! \"hehehe\"'\n",
    "a = a.replace('\"', '')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Aah Aah The games you played were never fun Yo...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>The lights go out and I can't be saved Tides t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>'Cause you're a sky, 'cause you're a sky full ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>I will not make the same mistakes that you did...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>Heyy Heeey Heey Your lipstick stains On the fr...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16203</th>\n",
       "      <td>Yeah, you! Yeah, you! I used to wanna be Livin...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16284</th>\n",
       "      <td>If you don't wanna see me Did a full 180, craz...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16771</th>\n",
       "      <td>Nah, nah, nah Cake by the ocean Oh, no See you...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17046</th>\n",
       "      <td>Come on, come on, turn the radio on It's Frida...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18402</th>\n",
       "      <td>Clock strikes upon the hour And the sun begins...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  lyrics embedding\n",
       "160    Aah Aah The games you played were never fun Yo...          \n",
       "445    The lights go out and I can't be saved Tides t...          \n",
       "763    'Cause you're a sky, 'cause you're a sky full ...          \n",
       "856    I will not make the same mistakes that you did...          \n",
       "1169   Heyy Heeey Heey Your lipstick stains On the fr...          \n",
       "...                                                  ...       ...\n",
       "16203  Yeah, you! Yeah, you! I used to wanna be Livin...          \n",
       "16284  If you don't wanna see me Did a full 180, craz...          \n",
       "16771  Nah, nah, nah Cake by the ocean Oh, no See you...          \n",
       "17046  Come on, come on, turn the radio on It's Frida...          \n",
       "18402  Clock strikes upon the hour And the sun begins...          \n",
       "\n",
       "[317 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Cause you're a sky, 'cause you're a sky full of stars I'm gonna give you my heart 'Cause you're a sky, 'cause you're a sky full of stars 'Cause you light up the path I don't care, go on and tear me apart I don't care if you do, ooh-ooh, ooh'Cause in a sky, 'cause in a sky full of stars I think I saw you 'Cause you're a sky, 'cause you're a sky full of stars I wanna die in your arms, oh, oh, oh'Cause you get lighter the more it gets dark I'm gonna give you my heart, oh I don't care, go on and tear me apart I don't care if you do, ooh-ooh, ooh'Cause in a sky, 'cause in a sky full of stars I think I see you I think I see you 'Cause you're a sky, you're a sky full of stars Such a heavenly view You're such a heavenly view (Yeah, yeah, yeah, ooh) 'Cause you're a sky, 'cause you're a sky full of stars I'm gonna give you my heart 'Cause you're a sky, 'cause you're a sky full of stars 'Cause you light up the path I don't care, go on and tear me apart I don't care if you do, ooh-ooh, ooh'Cause in a sky, 'cause in a sky full of stars I think I saw you 'Cause you're a sky, 'cause you're a sky full of stars I wanna die in your arms, oh, oh, oh'Cause you get lighter the more it gets dark I'm gonna give you my heart, oh I don't care, go on and tear me apart I don't care if you do, ooh-ooh, ooh'Cause in a sky, 'cause in a sky full of stars I think I see you I think I see you 'Cause you're a sky, you're a sky full of stars Such a heavenly view You're such a heavenly view (Yeah, yeah, yeah, ooh) 'Cause you're a sky, 'cause you're a sky full of stars I'm gonna give you my heart 'Cause you're a sky, 'cause you're a sky full of stars 'Cause you light up the path I don't care, go on and tear me apart I don't care if you do, ooh-ooh, ooh'Cause in a sky, 'cause in a sky full of stars I think I saw you 'Cause you're a sky, 'cause you're a sky full of stars I wanna die in your arms, oh, oh, oh'Cause you get lighter the more it gets dark I'm gonna give you my heart, oh I don't care, go on and tear me apart I don't care if you do, ooh-ooh, ooh'Cause in a sky, 'cause in a sky full of stars I think I see you I think I see you 'Cause you're a sky, you're a sky full of stars Such a heavenly view You're such a heavenly view (Yeah, yeah, yeah, ooh)\n",
      "torch.Size([768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([768])\n",
      "1 torch.Size([768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (674) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 674].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model(torch.tensor([tok.encode(lyrics_data['lyrics'].iloc[1])])).pooler_output.squeeze()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lyrics_data)):\n\u001b[0;32m---> 15\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtok\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlyrics_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlyrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpooler_output\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(idx, element\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m     lyrics_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[idx] \u001b[38;5;241m=\u001b[39m element\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1515'>1516</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1516'>1517</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1517'>1518</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1521'>1522</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1522'>1523</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1523'>1524</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1524'>1525</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1525'>1526</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1526'>1527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1528'>1529</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1529'>1530</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:801\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py?line=798'>799</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings, \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py?line=799'>800</a>\u001b[0m     buffered_token_type_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py?line=800'>801</a>\u001b[0m     buffered_token_type_ids_expanded \u001b[39m=\u001b[39m buffered_token_type_ids\u001b[39m.\u001b[39;49mexpand(batch_size, seq_length)\n\u001b[1;32m    <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py?line=801'>802</a>\u001b[0m     token_type_ids \u001b[39m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    <a href='file:///~/opt/miniconda3/envs/BKMS/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py?line=802'>803</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (674) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 674].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, pipeline\n",
    "import torch\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "config.output_hidden_states = True\n",
    "tok = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = RobertaModel.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config)\n",
    "\n",
    "# extract embedding\n",
    "print(lyrics_data['lyrics'].iloc[2])\n",
    "print(model(torch.tensor([tok.encode(lyrics_data['lyrics'].iloc[3])])).pooler_output.squeeze().detach().shape)\n",
    "# model(torch.tensor([tok.encode(lyrics_data['lyrics'].iloc[1])])).pooler_output.squeeze()\n",
    "for idx in range(len(lyrics_data)):\n",
    "    element = model(torch.tensor([tok.encode(lyrics_data['lyrics'].iloc[idx])])).pooler_output.squeeze().detach()\n",
    "    print(idx, element.shape)\n",
    "    lyrics_data['embedding'].iloc[idx] = element\n",
    "# lyrics_data['embedding'] = lyrics_data['lyrics'].apply(lambda e: model(torch.tensor([tok.encode(e)])).pooler_output.squeeze())\n",
    "lyrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./lyrics_embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(lyrics_data, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1367c8c4473d3265912c8d7e3cdd5911d2a91ae2eeda60059a3a8b4e60ae8f13"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 ('BKMS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
